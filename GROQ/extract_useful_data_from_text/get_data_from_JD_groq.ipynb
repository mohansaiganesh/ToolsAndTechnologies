{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=\"Your api key\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_JD_groq(job_description, prompt, model_name = \"llama3-8b-8192\"):\n",
    "    global client\n",
    "    # return the final prompt for given job description\n",
    "    def get_final_prompt(job_description,prompt):\n",
    "        return f'\"{job_description}\" \\n{prompt} '\n",
    "\n",
    "    #write a logic to avoid ratelimter - PENDING\n",
    "    response = client.chat.completions.create(\n",
    "        model= model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": get_final_prompt(job_description, prompt)}],\n",
    "        temperature=1,\n",
    "        top_p=1,\n",
    "        stream=False,\n",
    "        stop=None,\n",
    "    )\n",
    "\n",
    "    message_content = response.choices[0].message.content\n",
    "\n",
    "    # print(message_content)\n",
    "    tokens = response.usage.total_tokens\n",
    "    print('number of tokens:',tokens)\n",
    "\n",
    "    return message_content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
